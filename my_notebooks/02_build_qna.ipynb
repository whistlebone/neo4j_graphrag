{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7344bcd",
   "metadata": {},
   "source": [
    "### Import libraries and configure settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e4a11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "\n",
    "from langchain.chains.qa_with_sources.retrieval import RetrievalQAWithSourcesChain\n",
    "from langchain_neo4j.vectorstores.neo4j_vector import Neo4jVector\n",
    "from langchain_neo4j.chains.graph_qa.cypher import GraphCypherQAChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from src.factory.llm import fetch_llm\n",
    "from src.config import Source, ChunkerConf, LLMConf, EmbedderConf, KnowledgeGraphConfig\n",
    "from src.graph.knowledge_graph import KnowledgeGraph\n",
    "from src.ingestion.embedder import ChunkEmbedder\n",
    "\n",
    "env = load_dotenv('config.env', override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7d979c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_config = KnowledgeGraphConfig(\n",
    "    uri=os.getenv(\"NEO4J_URI\"),\n",
    "    user=os.getenv(\"NEO4J_USERNAME\"),\n",
    "    password=os.getenv(\"NEO4J_PASSWORD\"),\n",
    "    index_name=\"vector\"\n",
    ")\n",
    "\n",
    "chunker_conf = ChunkerConf(\n",
    "    type=\"recursive\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "llm_conf = LLMConf(\n",
    "    model=os.getenv(\"AZURE_OPENAI_LLM_MODEL_NAME\"),\n",
    "    temperature=0,\n",
    "    type=\"azure-openai\",\n",
    "    deployment=os.getenv(\"AZURE_OPENAI_LLM_DEPLOYMENT_NAME\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    endpoint=os.getenv(\"AZURE_OPENAI_LLM_ENDPOINT\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_LLM_VERSION\"),\n",
    ")\n",
    "\n",
    "embedder_conf = EmbedderConf(\n",
    "    model=os.getenv(\"AZURE_OPENAI_EMBEDDING_MODEL_NAME\"),\n",
    "    type=\"azure-openai\",\n",
    "    deployment=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    endpoint=os.getenv(\"AZURE_OPENAI_EMBEDDING_ENDPOINT\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_EMBEDDING_VERSION\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e97cb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 10:37:05,140 - src.ingestion.embedder - INFO - Embedder of type 'ModelType.AZURE_OPENAI' initialized.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder = ChunkEmbedder(conf=embedder_conf)\n",
    "knowledge_graph = KnowledgeGraph(\n",
    "    conf=kg_config, \n",
    "    embeddings_model=embedder.embeddings\n",
    ")\n",
    "knowledge_graph._driver.verify_connectivity()\n",
    "knowledge_graph._driver.verify_authentication()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ccc0a0",
   "metadata": {},
   "source": [
    "### Query against both vector store and knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "03e2da8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the same query to test against\n",
    "query = \"Where was Lucy born?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456af127",
   "metadata": {},
   "source": [
    "### Vanilla RAG (aka not using graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d80bcda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 10:55:08,323 - src.factory.llm - INFO - Fetching LLM model 'gpt-4o-mini'..\n",
      "2026-01-13 10:55:08,908 - src.factory.llm - INFO - Initialized LLM of type: 'ModelType.AZURE_OPENAI'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but the provided context does not mention anyone named Lucy or where she was born. It only discusses Marco Rossi and his background in Rome, Italy. If you have more information or another question, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "prompt=\"\"\"\n",
    "You are a helpful assistant. Write clear, natural, and concise answers.\n",
    "Ground your response primarily in the provided CONTEXT. If the context is insufficient\n",
    "to answer the question, say so briefly and indicate what's missing. Follow safety and\n",
    "content policies at all times.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "Instructions:\n",
    "- Use only what's relevant from the CONTEXT to answer the QUESTION.\n",
    "- Do not invent facts beyond the CONTEXT.\n",
    "- If needed, say you don't have enough information.\n",
    "- Write in a friendly, human tone.\n",
    "\"\"\"\n",
    "template=PromptTemplate.from_template(prompt)\n",
    "template.input_variables=[\"context\", \"question\"]\n",
    "\n",
    "retriever = knowledge_graph.vector_store.as_retriever(\n",
    "    search_type=\"similarity\", \n",
    "    search_kwargs={\"k\":5}\n",
    ")\n",
    "\n",
    "llm = fetch_llm(conf=llm_conf)\n",
    "\n",
    "response = llm.invoke(\n",
    "    input=prompt.format(context=retriever.invoke(query), question=query), \n",
    ")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24769be7",
   "metadata": {},
   "source": [
    "### CQL Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3429de94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_neo4j import Neo4jGraph, GraphCypherQAChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c428bf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Your task is to rephrase a user's question based on the schema of a graph database that will be given to you. \n",
    "\n",
    "Do not mention anything else, just rephrase the question from the user to be as ccherent as possible with the schema of the graph.\n",
    "Do not make things up or add any information on your own. \n",
    "\n",
    "SCHEMA: {schema}\n",
    "QUESTION: {question}\n",
    "\n",
    "REPHRASED_QUESTION: \n",
    "\"\"\"\n",
    "template=PromptTemplate.from_template(prompt)\n",
    "template.input_variables = ['schema', 'question']\n",
    "template.partial_variables = {\"schema\": knowledge_graph.get_structured_schema}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "caec0a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In which city or country was Lucy born?'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rephrased_question = llm.invoke(input=template.format(question=query)).content\n",
    "rephrased_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da903e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new GraphCypherQAChain chain...\u001b[0m\n",
      "Generated Cypher:\n",
      "\u001b[32;1m\u001b[1;3mcypher\n",
      "MATCH (p:Person {name: 'Lucy'})-[:BORN_IN]->(location)\n",
      "RETURN location.name AS birthplace\n",
      "\u001b[0m\n",
      "Full Context:\n",
      "\u001b[32;1m\u001b[1;3m[]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'In which city or country was Lucy born?',\n",
       " 'result': \"I don't know the answer.\"}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_chain = GraphCypherQAChain.from_llm(\n",
    "    llm=llm, \n",
    "    graph=knowledge_graph, \n",
    "    verbose=True,\n",
    "    allow_dangerous_requests=True,\n",
    "    validate_cypher=True\n",
    ")\n",
    "graph_chain.invoke(rephrased_question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
