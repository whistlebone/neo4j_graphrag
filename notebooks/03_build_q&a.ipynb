{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "\n",
    "from langchain.chains.qa_with_sources.retrieval import RetrievalQAWithSourcesChain\n",
    "from langchain_neo4j.vectorstores.neo4j_vector import Neo4jVector\n",
    "from langchain_neo4j.chains.graph_qa.cypher import GraphCypherQAChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from src.factory.llm import fetch_llm\n",
    "from src.config import Source, ChunkerConf, LLMConf, EmbedderConf, KnowledgeGraphConfig\n",
    "from src.graph.knowledge_graph import KnowledgeGraph\n",
    "from src.ingestion.embedder import ChunkEmbedder\n",
    "\n",
    "env = load_dotenv('config.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_config = KnowledgeGraphConfig(\n",
    "    uri=os.getenv(\"NEO4J_URI\"),\n",
    "    user=os.getenv(\"NEO4J_USERNAME\"),\n",
    "    password=os.getenv(\"NEO4J_PASSWORD\"),\n",
    "    index_name='vector'\n",
    ")\n",
    "\n",
    "llm_conf = LLMConf(\n",
    "    type=\"ollama\",\n",
    "    model=\"llama3.2:latest\", \n",
    "    temperature=0.0, \n",
    ")\n",
    "\n",
    "embedder_conf = EmbedderConf(\n",
    "    type=\"ollama\",\n",
    "    model=\"mxbai-embed-large\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = ChunkEmbedder(conf=embedder_conf)\n",
    "\n",
    "knowledge_graph = KnowledgeGraph(\n",
    "    conf=kg_config, \n",
    "    embeddings_model=embedder.embeddings\n",
    ")\n",
    "\n",
    "knowledge_graph._driver.verify_connectivity()\n",
    "\n",
    "knowledge_graph._driver.verify_authentication()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Which document mentions â‚¬300 billion in RRF payments?\"\n",
    "knowledge_graph.vector_store.similarity_search(query=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basic_rag_prompt() -> PromptTemplate:\n",
    "\n",
    "    prompt = \"\"\"\n",
    "        You are an assistant that helps to form nice and human understandable answers.\n",
    "        The information part contains the provided information that you must use to construct an answer.\n",
    "        The provided information is authoritative, you must never doubt it or try to use your internal knowledge to correct it.\n",
    "        Make the answer sound as a response to the question. Do not mention that you based the result on the given information.\n",
    "\n",
    "        CONTEXT: {context}\n",
    "        QUESTION: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    template = PromptTemplate.from_template(prompt)\n",
    "\n",
    "    template.input_variables = ['context', 'question']\n",
    "\n",
    "    return template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Which Countries have received RRF payments?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_prompt = get_basic_rag_prompt()\n",
    "\n",
    "retriever = knowledge_graph.vector_store.as_retriever(\n",
    "    search_type=\"similarity\", \n",
    "    search_kwargs={\"k\":5}\n",
    ")\n",
    "\n",
    "llm = fetch_llm(conf=llm_conf)\n",
    "\n",
    "response = llm.invoke(\n",
    "    input=basic_prompt.format(context=retriever.invoke(query), question=query), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cypher Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_neo4j import Neo4jGraph, GraphCypherQAChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_chain = GraphCypherQAChain.from_llm(\n",
    "    llm=llm, \n",
    "    graph=knowledge_graph, \n",
    "    verbose=True,\n",
    "    allow_dangerous_requests=True,\n",
    "    validate_cypher=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_chain.invoke(\"What countries are mentioned in the graph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_chain.invoke(\"What amount of money was disboursed to Romania?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rephrase Prompt\n",
    "add an agent that rephrases the user question given the graph schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rephrase_prompt() -> PromptTemplate:\n",
    "\n",
    "    prompt = \"\"\"\n",
    "        Your task is to rephrase a user's question based on the schema of a graph database that will be given to you. \n",
    "        \n",
    "        Do not mention anything else, just rephrase the question from the user to be as ccherent as possible with the schema of the graph.\n",
    "        Do not make things up or add any information on your own. \n",
    "\n",
    "        SCHEMA: {schema}\n",
    "        QUESTION: {question}\n",
    "\n",
    "        REPHRASED_QUESTION: \n",
    "    \"\"\"\n",
    "\n",
    "    template = PromptTemplate.from_template(prompt)\n",
    "\n",
    "    template.input_variables = ['schema', 'question']\n",
    "\n",
    "    return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"When was the request for payment from Germany submitted?\"\n",
    "\n",
    "rephrase_prompt = get_rephrase_prompt()\n",
    "\n",
    "rephrase_prompt.partial_variables = {\"schema\": knowledge_graph.get_structured_schema}\n",
    "\n",
    "rephrased_question = llm.invoke(input=rephrase_prompt.format(question=query)).content\n",
    "\n",
    "rephrased_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_chain.invoke(rephrased_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined Approach\n",
    "We want to try what happens if we give to a third agent the task to summarize results from both a Cypher Query (if any result) AND a Vector Search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rephrase_prompt() -> PromptTemplate:\n",
    "\n",
    "    prompt = \"\"\"\n",
    "        Your task is to rephrase a user's question based on the schema of a Graph Database that will be given to you. \n",
    "\n",
    "        Do not mention anything else, just rephrase the question from the user to be as ccherent as possible with the schema of the graph.\n",
    "        Do not make things up or add any information on your own. \n",
    "\n",
    "        SCHEMA: {schema}\n",
    "        QUESTION: {question}\n",
    "\n",
    "        REPHRASED_QUESTION: \n",
    "    \"\"\"\n",
    "\n",
    "    template = PromptTemplate.from_template(prompt)\n",
    "\n",
    "    template.input_variables = ['schema', 'question']\n",
    "\n",
    "    return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summarization_prompt() -> PromptTemplate:\n",
    "\n",
    "    prompt = \"\"\"\n",
    "        Your task is to synthetize a clear and helpful answer to a question.\n",
    "\n",
    "        The information to use for the task come from a Vector Database and from a Graph Database.\n",
    "        \n",
    "        In your task, you MUST use either the context obtained from a vector search on the Vector Database \n",
    "        and the query results given running a Cypher Query on the Graph Database. I\n",
    "\n",
    "        Do not mention anything else, just summarize an precise, clear and helpful answer. \n",
    "        Do not make things up or add any information on your own. \n",
    "\n",
    "        QUESTION: {question}\n",
    "\n",
    "        RETRIEVED CONTEXT: {retrieved_context}\n",
    "\n",
    "        QUERY RESULT ON GRAPH: {query_result}\n",
    "\n",
    "        ANSWER: \n",
    "    \"\"\"\n",
    "\n",
    "    template = PromptTemplate.from_template(prompt)\n",
    "\n",
    "    template.input_variables = ['question', 'retrieved_context', 'query_result']\n",
    "\n",
    "    return template\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_prompt = get_summarization_prompt()\n",
    "\n",
    "rephrase_prompt = get_rephrase_prompt()\n",
    "\n",
    "graph_chain = GraphCypherQAChain.from_llm(\n",
    "    llm=llm, \n",
    "    graph=knowledge_graph, \n",
    "    verbose=True,\n",
    "    allow_dangerous_requests=True,\n",
    "    validate_cypher=True, \n",
    "    return_intermediate_steps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How much money was received by Portugal and why?\"\n",
    "\n",
    "context_docs = knowledge_graph.vector_store.similarity_search(query=query)\n",
    "\n",
    "context = \"\"\n",
    "\n",
    "for doc in context_docs:\n",
    "    context += f\"\\n {doc.page_content}\"\n",
    "\n",
    "# rephrase_prompt.partial_variables = {\"schema\": knowledge_graph.get_structured_schema}\n",
    "\n",
    "# rephrased_question = llm.invoke(input=rephrase_prompt.format(question=query)).content\n",
    "\n",
    "knowledge_graph._driver.verify_connectivity()\n",
    "\n",
    "try:\n",
    "    graph_qa_output = graph_chain.invoke(query)\n",
    "except Exception as e:\n",
    "    graph_qa_output = None\n",
    "\n",
    "final_answer = llm.invoke(\n",
    "    input=summarization_prompt.format(\n",
    "        question=query, \n",
    "        retrieved_context=context, \n",
    "        query_result=graph_qa_output['intermediate_steps'] if graph_qa_output else {}\n",
    "    )\n",
    ")\n",
    "\n",
    "final_answer.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
